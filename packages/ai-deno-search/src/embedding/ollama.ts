import { type EmbedRequest, Ollama } from "ollama";
import { Embedding, type EmbeddingVector } from "../types.ts";

export interface OllamaEmbeddingConfig {
  model: string;
  host?: string;
  fetch?: typeof globalThis.fetch;
  keepAlive?: string | number;
  options?: Record<string, unknown>;
  dimension?: number;
  maxTokens?: number;
}

export class OllamaEmbedding extends Embedding {
  private client: Ollama;
  private config: OllamaEmbeddingConfig;
  private dimension: number = 768;
  private dimensionDetected: boolean = false;
  protected maxTokens: number = 2048;

  constructor(config: OllamaEmbeddingConfig) {
    super();
    this.config = config;
    this.client = new Ollama({
      host: config.host || "http://127.0.0.1:11434",
      fetch: config.fetch,
    });

    if (config.dimension) {
      this.dimension = config.dimension;
      this.dimensionDetected = true;
    }

    if (config.maxTokens) {
      this.maxTokens = config.maxTokens;
    } else {
      this.setDefaultMaxTokensForModel(config.model);
    }
  }

  private setDefaultMaxTokensForModel(model: string): void {
    if (model?.includes("nomic-embed-text")) {
      this.maxTokens = 8192;
    } else if (model?.includes("snowflake-arctic-embed")) {
      this.maxTokens = 8192;
    } else {
      this.maxTokens = 2048;
    }
  }

  async embed(text: string): Promise<EmbeddingVector> {
    const processedText = this.preprocessText(text);

    if (!this.dimensionDetected && !this.config.dimension) {
      this.dimension = await this.detectDimension();
      this.dimensionDetected = true;
      console.log(
        `[OllamaEmbedding] üìè Detected Ollama embedding dimension: ${this.dimension} for model: ${this.config.model}`,
      );
    }

    const embedOptions: EmbedRequest = {
      model: this.config.model,
      input: processedText,
      options: this.config.options,
    };

    if (this.config.keepAlive && this.config.keepAlive !== "") {
      embedOptions.keep_alive = this.config.keepAlive;
    }

    const response = await this.client.embed(embedOptions);

    if (!response.embeddings || !response.embeddings[0]) {
      throw new Error("Ollama API returned invalid response");
    }

    return {
      vector: response.embeddings[0],
      dimension: this.dimension,
    };
  }

  async embedBatch(texts: string[]): Promise<EmbeddingVector[]> {
    const processedTexts = this.preprocessTexts(texts);

    if (!this.dimensionDetected && !this.config.dimension) {
      this.dimension = await this.detectDimension();
      this.dimensionDetected = true;
      console.log(
        `[OllamaEmbedding] üìè Detected Ollama embedding dimension: ${this.dimension} for model: ${this.config.model}`,
      );
    }

    const embedOptions: EmbedRequest = {
      model: this.config.model,
      input: processedTexts,
      options: this.config.options,
    };

    if (this.config.keepAlive && this.config.keepAlive !== "") {
      embedOptions.keep_alive = this.config.keepAlive;
    }

    const response = await this.client.embed(embedOptions);

    if (!response.embeddings || !Array.isArray(response.embeddings)) {
      throw new Error("Ollama API returned invalid batch response");
    }

    return response.embeddings.map((embedding: number[]) => ({
      vector: embedding,
      dimension: this.dimension,
    }));
  }

  getDimension(): number {
    return this.dimension;
  }

  getProvider(): string {
    return "Ollama";
  }

  async setModel(model: string): Promise<void> {
    this.config.model = model;
    this.dimensionDetected = false;
    this.setDefaultMaxTokensForModel(model);
    if (!this.config.dimension) {
      this.dimension = await this.detectDimension();
      this.dimensionDetected = true;
      console.log(
        `[OllamaEmbedding] üìè Detected Ollama embedding dimension: ${this.dimension} for model: ${this.config.model}`,
      );
    } else {
      console.log(
        "[OllamaEmbedding] Dimension already detected for model " +
          this.config.model,
      );
    }
  }

  setHost(host: string): void {
    this.config.host = host;
    this.client = new Ollama({
      host: host,
      fetch: this.config.fetch,
    });
  }

  setKeepAlive(keepAlive: string | number): void {
    this.config.keepAlive = keepAlive;
  }

  setOptions(options: Record<string, unknown>): void {
    this.config.options = options;
  }

  setMaxTokens(maxTokens: number): void {
    this.config.maxTokens = maxTokens;
    this.maxTokens = maxTokens;
  }

  getClient(): Ollama {
    return this.client;
  }

  async detectDimension(testText: string = "test"): Promise<number> {
    console.log(`[OllamaEmbedding] Detecting embedding dimension...`);

    try {
      const processedText = this.preprocessText(testText);
      const embedOptions: EmbedRequest = {
        model: this.config.model,
        input: processedText,
        options: this.config.options,
      };

      if (this.config.keepAlive && this.config.keepAlive !== "") {
        embedOptions.keep_alive = this.config.keepAlive;
      }

      const response = await this.client.embed(embedOptions);

      if (!response.embeddings || !response.embeddings[0]) {
        throw new Error("Ollama API returned invalid response");
      }

      const dimension = response.embeddings[0].length;
      console.log(
        `[OllamaEmbedding] Successfully detected embedding dimension: ${dimension}`,
      );
      return dimension;
    } catch (error) {
      const errorMessage = error instanceof Error
        ? error.message
        : "Unknown error";
      console.error(
        `[OllamaEmbedding] Failed to detect dimension: ${errorMessage}`,
      );
      throw new Error(
        `Failed to detect Ollama embedding dimension: ${errorMessage}`,
      );
    }
  }
}
